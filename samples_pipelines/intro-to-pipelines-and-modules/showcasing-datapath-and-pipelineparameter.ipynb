{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcasing GlobalDataset and PipelineParameter\n",
    "\n",
    "This notebook demonstrateas the use of **GlobalDataset** and **PipelineParameters** in AML Pipeline. You will learn how strings and **GlobalDataset** can be parameterized and submitted to AML Pipelines via **PipelineParameters**.\n",
    "To see more about how parameters work between steps, please refer [aml-pipelines-with-data-dependency-steps](https://aka.ms/pl-data-dep).\n",
    "\n",
    "* [How to create a Pipeline with a PipelineParameter](#index1)\n",
    "* [How to submit a Pipeline with a PipelineParameter](#index2)\n",
    "* [How to submit a Pipeline and change the PipelineParameter value from the sdk](#index3)\n",
    "* [How to submit a Pipeline and change the PipelineParameter value using a REST call](#index4)\n",
    "* [How to create a datastore trigger schedule and use the data_path_parameter_name to get the path of the changed blob in the Pipeline](#index5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "from azureml.pipeline.wrapper import Module, Pipeline, PipelineRun, dsl\n",
    "from azureml.pipeline.wrapper._dataset import _GlobalDataset\n",
    "from azureml.pipeline.wrapper._pipeline_parameters import PipelineParameter\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Login to azure with cli and set the default workspace using `az ml folder attach` command.\n",
    "\n",
    "After this operation, the workspace could be retrived with the `Workspace.from_config()` for SDK usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Update the following information with your environment\n",
    "\n",
    "SUBSCRIPTION_ID = '<your subscription ID>'\n",
    "WORKSPACE_NAME = '<your workspace name>'\n",
    "RESOURCE_GROUP_NAME = '<your resource group>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!az login -o none \n",
    "!az account set -s $SUBSCRIPTION_ID \n",
    "!az ml folder attach -w $WORKSPACE_NAME -g $RESOURCE_GROUP_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "\n",
    "Let's create an experiment named \"automl-classification\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "    print('Found existing compute target {}.'.format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"Standard_D2_v2\",\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, timeout_in_minutes=20)\n",
    "\n",
    "print(\"Azure Machine Learning Compute attached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and arguments setup \n",
    "\n",
    "We use a Select Column Module to illustrate how to use Datapath and PipelinePrameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'titanic-cleaned.csv'\n",
    "\n",
    "# get dataset\n",
    "titanic_dataset = Dataset.get_by_name(workspace, dataset_name)\n",
    "\n",
    "# module parameters\n",
    "select_columns = \"{\\\"isFilter\\\":true,\\\"rules\\\":[{\\\"exclude\\\":false,\\\"ruleType\\\":\\\"AllColumns\\\"}]}\"\n",
    "\n",
    "# use 'Select Columns in Dataset' as the sample module\n",
    "select_column_module_func = Module.load(workspace, namespace='azureml', name='Select Columns in Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline with PipelineParameter\n",
    "\n",
    "Create a module by assigning its parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a module\n",
    "select_column_module = select_column_module_func(dataset=titanic_dataset, select_columns=select_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a pipeline\n",
    "pipeline = Pipeline(nodes=[select_column_module],\n",
    "                    workspace=workspace,\n",
    "                    name=\"select-column-sample-pipeline\",\n",
    "                    description=\"pipeline for GlobalDataset-PipelineParameter sample usage\",\n",
    "                    default_compute_target=cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Choose a name for the run history container in the workspace.\n",
    "experiment_name = 'showcasing-GlobalDataset-PipelineParameter'\n",
    "pipeline_run = pipeline.submit(experiment_name=experiment_name)\n",
    "\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline with PipelineParameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DataPath PipelineParameter\n",
    "datastore_name = 'workspaceblobstore'\n",
    "datapath = _GlobalDataset(workspace=workspace, data_store_name=datastore_name, relative_path=dataset_name)\n",
    "\n",
    "datapath_parameter = PipelineParameter(name='datapath', default_value=datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline parameter\n",
    "pipeline_parameter = PipelineParameter(name='select_columns', default_value=select_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a pipeline function\n",
    "@dsl.pipeline(name='select-column-sample-pipeline',\n",
    "              description='pipeline for GlobalDataset-PipelineParameter sample usage',\n",
    "              default_compute_target=cluster_name)\n",
    "def sample_pipeline(input, _select_columns):\n",
    "    select_column_module = select_column_module_func(dataset=input,\n",
    "                                                     select_columns=_select_columns)\n",
    "    return select_column_module.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline using PipelineParameter\n",
    "pipeline = sample_pipeline(input=datapath_parameter, _select_columns=pipeline_parameter)\n",
    "print(\"Pipeline is created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a Pipeline with a GlobalDataset PipelineParameter\n",
    "\n",
    "Pipelines can be submitted with default values of PipelineParameters by not specifying any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run = pipeline.submit(experiment_name=experiment_name)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a Pipeline and change the PipelineParameter value from the sdk\n",
    "\n",
    "Or Pipelines can be submitted with values other than default ones by using pipeline_parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = sample_pipeline(input=None, _select_columns=None)\n",
    "pipeline_run_with_params = pipeline.submit(experiment_name=experiment_name,\n",
    "                                           pipeline_parameters={'input': titanic_dataset,\n",
    "                                                                '_select_columns': select_columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run_with_params.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run_with_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a Pipeline and change the PipelineParameter value using a REST call\n",
    "\n",
    "Let's published the pipeline to use the rest endpoint of the published pipeline. We publish a pipeline using PipelineEndpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.wrapper import PipelineEndpoint\n",
    "\n",
    "pipeline = sample_pipeline(input=titanic_dataset, _select_columns=select_columns)\n",
    "\n",
    "# publish pipeline to an endpoint named \"PipelineParameterTest\", make it as default version.\n",
    "pipeline_endpoint = PipelineEndpoint.publish(workspace=workspace, name=\"PipelineParameterTest\",\n",
    "                                             pipeline=pipeline, description=\"Test description Notebook\", \n",
    "                                             set_as_default=True)\n",
    "\n",
    "pipeline_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the published pipeline by default version\n",
    "default_version = pipeline_endpoint.default_version\n",
    "print('Default version of \"PipelineParameterTest\" endpoint is:', default_version)\n",
    "\n",
    "# get the list of published pipeline in PipelineParameterTest endpoint\n",
    "published_pipeline_list = pipeline_endpoint.list_pipelines()\n",
    "print('published_pipeline_list:\\n', published_pipeline_list)\n",
    "\n",
    "# fetch the published pipeline\n",
    "published_pipeline = published_pipeline_list[default_version]\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def_blob_store = workspace.get_default_datastore()\n",
    "print(\"Default datastore's name: {}\".format(def_blob_store.name))\n",
    "\n",
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"MyRestPipeline\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                               \"DataPathAssignments\": {\n",
    "                                   \"input\": { \n",
    "                                       \"DataStoreName\": def_blob_store.name\n",
    "                                   }\n",
    "                               },\n",
    "                               \"ParameterAssignments\": {\"input_string\": \"sample_string3\"}\n",
    "                              }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest = PipelineRun(workspace.experiments[\"MyRestPipeline\"], run_id)\n",
    "published_pipeline_run_via_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Datastore trigger schedule and use data path parameter\n",
    "\n",
    "When the Pipeline is scheduled with GlobalDataset parameter, it will be triggered by the modified or added data in the GlobalDataset. ```path_on_datastore``` should be a folder and the value of the GlobalDataset will be replaced by the path of the modified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "schedule = Schedule.create(workspace=workspace, \n",
    "                           name=\"Datastore_trigger_schedule\",\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name='Scheduled_Pipeline',\n",
    "                           datastore=def_blob_store,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"Datastore trigger schedule demo\",\n",
    "                           path_on_datastore=\"sample_datapath_for_folder\",\n",
    "                           data_path_parameter_name=\"input\") #Same name as used above to create PipelineParameter\n",
    "\n",
    "print(\"Created schedule with id: {}\".format(schedule.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "schedule.disable()\n",
    "schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish\n",
    "Disable created PipelineEndpoint and PublishedPipeline in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint.disable()\n",
    "published_pipeline.disable()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "How to use DataPath as a PipelineParameter",
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "order_index": 13,
  "star_tag": [
   "featured"
  ],
  "tags": [
   "None"
  ],
  "task": "Demonstrates the use of DataPath as a PipelineParameter"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
