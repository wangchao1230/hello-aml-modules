{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-data-dependency-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcasing Dataset and Pipeline Parameter\n",
    "\n",
    "This notebook demonstrateas the usage of [**Dataset**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset(class)?view=azure-ml-py), more specifically [**FileDataset**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py), we don't support [**TabularDataset**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) for now. You will learn how **Dataset** and other parameters are submitted to AML Pipelines via **Pipeline Parameters**. By parametrizing datasets, you can dynamically run pipeline experiments with different datasets without any code change.\n",
    "\n",
    "* [How to create a Pipeline with Pipeline Parameter](#index1)\n",
    "* [How to submit a Pipeline with Pipeline Parameter](#index2)\n",
    "* [How to submit a Pipeline and change the Pipeline Parameter value from the sdk](#index3)\n",
    "* [How to submit a Pipeline and change the Pipeline Parameter value using a REST call](#index4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning and Pipeline SDK-specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "from azureml.pipeline.wrapper import Module, Pipeline, PipelineRun, dsl\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Login to azure with cli and set the default workspace using `az ml folder attach` command.\n",
    "\n",
    "After this operation, the workspace could be retrived with the `Workspace.from_config()` for SDK usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Update the following information with your environment\n",
    "\n",
    "SUBSCRIPTION_ID = '<your subscription ID>'\n",
    "WORKSPACE_NAME = '<your workspace name>'\n",
    "RESOURCE_GROUP_NAME = '<your resource group>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!az login -o none \n",
    "!az account set -s $SUBSCRIPTION_ID \n",
    "!az ml folder attach -w $WORKSPACE_NAME -g $RESOURCE_GROUP_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace_helpers import setup_default_workspace\n",
    "workspace = setup_default_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve or create an Azure Machine Learning compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "    print('Found existing compute target {}.'.format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"Standard_D2_v2\",\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True, timeout_in_minutes=20)\n",
    "\n",
    "print(\"Azure Machine Learning Compute attached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Arguments Setup\n",
    "\n",
    "The following steps illustrate how to get/create a [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) and [TabularDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) from an external CSV file, we use 'titanic-cleaned.csv' dataset as the sample dataset.\n",
    "\n",
    "We use Select Column Module as the sample module to illustrate how a pipeline is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get sample FileDataset\n",
    "dataset_name = 'titanic-cleaned.csv'\n",
    "titanic_dataset_get = Dataset.get_by_name(workspace, dataset_name)\n",
    "print('Datatype of file_dataset: {}'.format(type(titanic_dataset_get)))\n",
    "\n",
    "# create FileDataset\n",
    "titanic_dataset_created = Dataset.File.from_files('https://dprepdata.blob.core.windows.net/demo/Titanic.csv')\n",
    "print('Datatype of file_dataset: {}'.format(type(titanic_dataset_created)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 'Select Columns in Dataset' module function\n",
    "select_column_module_func = Module.load(workspace, namespace='azureml', name='Select Columns in Dataset')\n",
    "\n",
    "# define module parameters\n",
    "select_columns = \"{\\\"isFilter\\\":true,\\\"rules\\\":[{\\\"exclude\\\":false,\\\"ruleType\\\":\\\"AllColumns\\\"}]}\"\n",
    "\n",
    "# choose a name for the run history container in the workspace.\n",
    "experiment_name = 'showcasing-Dataset-PipelineParameter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline with Pipeline Parameter\n",
    "\n",
    "Create a module by assigning modules' parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a module\n",
    "select_column_module = select_column_module_func(dataset=titanic_dataset_get, # Or titanic_dataset_created\n",
    "                                                 select_columns=select_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a pipeline\n",
    "pipeline = Pipeline(nodes=[select_column_module],\n",
    "                    workspace=workspace,\n",
    "                    name=\"select-column-sample-pipeline\",\n",
    "                    description=\"pipeline for Dataset-PipelineParameter sample usage\",\n",
    "                    default_compute_target=cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# submit pipeline\n",
    "pipeline_run = pipeline.submit(experiment_name=experiment_name)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline using pipeline parameter. In the sample pipeline function below, pipeline parameters are:\n",
    "* input\n",
    "* _select_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a pipeline function\n",
    "@dsl.pipeline(name='select-column-sample-pipeline',\n",
    "              description='pipeline for GlobalDataset-PipelineParameter sample usage',\n",
    "              default_compute_target=cluster_name)\n",
    "def sample_pipeline(input, _select_columns):\n",
    "    print('Datatype of input: {}'.format(type(input)))\n",
    "    print('Datatype of _select_columns: {}'.format(type(_select_columns)))\n",
    "\n",
    "    select_column_module = select_column_module_func(dataset=input,\n",
    "                                                     select_columns=_select_columns)\n",
    "    return select_column_module.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline using pipeline parameter\n",
    "# dsl.pipeline will transfer inputs parameters into PipelineParameter datatype.\n",
    "pipeline = sample_pipeline(input=titanic_dataset_get, _select_columns=select_columns)\n",
    "print(\"Pipeline is created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='index2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submit a Pipeline with default Pipeline Parameters\n",
    "\n",
    "Pipelines can be submitted with default values of Pipeline Parameters by not specifying any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run = pipeline.submit(experiment_name=experiment_name)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='index3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submit a Pipeline and change the Pipeline Parameters value from the sdk\n",
    "\n",
    "Or Pipelines can be submitted with values other than default ones by using `pipeline_parameters`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define a pipeline with None input\n",
    "pipeline = sample_pipeline(input=None, _select_columns=None)\n",
    "# update pipeline parameters when submit using 'pipeline_parameters'\n",
    "pipeline_run_with_params = pipeline.submit(experiment_name=experiment_name,\n",
    "                                           pipeline_parameters={'input': titanic_dataset_get,\n",
    "                                                                '_select_columns': select_columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run_with_params.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_run_with_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='index4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submit a Pipeline and change the Pipeline Parameter value using a REST call\n",
    "\n",
    "Let's published the pipeline to use the rest endpoint of the published pipeline. We publish a pipeline using **PipelineEndpoint**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.wrapper import PipelineEndpoint\n",
    "\n",
    "pipeline = sample_pipeline(input=titanic_dataset_get, _select_columns=select_columns)\n",
    "\n",
    "# publish pipeline to an endpoint named \"PipelineParameterTest\", and make it as default version.\n",
    "pipeline_endpoint = PipelineEndpoint.publish(workspace=workspace, name=\"PipelineParameterTest\",\n",
    "                                             pipeline=pipeline, description=\"Test description Notebook\", \n",
    "                                             set_as_default=True)\n",
    "\n",
    "pipeline_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fetch the published pipeline by default version\n",
    "default_version = pipeline_endpoint.default_version\n",
    "print('Default version of \"PipelineParameterTest\" endpoint is: {} \\n'.format(default_version))\n",
    "\n",
    "# get the list of published pipeline in PipelineParameterTest endpoint\n",
    "published_pipeline_list = pipeline_endpoint.list_pipelines(active_only=True)\n",
    "print('Published pipeline list:\\n {}'.format(published_pipeline_list))\n",
    "\n",
    "# fetch the published pipeline\n",
    "published_pipeline = published_pipeline_list[default_version]\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "\n",
    "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's change a dataset\n",
    "automobile_price_dataset = Dataset.get_by_name(workspace, 'Automobile_price_data_(Raw)')\n",
    "\n",
    "# meanwile, we update \"select_columns\" parameter, \"normalized-losses\" is a column name in automobile_price_dataset\n",
    "select_columns = \"{\\\"isFilter\\\":true,\\\"rules\\\":[{\\\"exclude\\\":false,\\\"ruleType\\\":\\\"AllColumns\\\"},\" \\\n",
    "                 \"{\\\"exclude\\\":true,\\\"ruleType\\\":\\\"ColumnNames\\\",\\\"columns\\\":[\\\"normalized-losses\\\"]}]}\"\n",
    "\n",
    "# specify the param when running the pipeline\n",
    "# NOTE: parameter name \"input\" and \"_select_columns\" should be the same with these in pipeline defination parameters\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"MyRestPipeline\",\n",
    "                               \"RunSource\": \"SDK\",\n",
    "                                \"DataSetDefinitionValueAssignments\": {\n",
    "                                    \"input\": {\n",
    "                                        \"SavedDataSetReference\": {\"Id\": automobile_price_dataset.id}\n",
    "                                    }\n",
    "                                },\n",
    "                               \"ParameterAssignments\": {\"_select_columns\": select_columns}\n",
    "                              }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:\n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest = PipelineRun(workspace.experiments[\"MyRestPipeline\"], run_id)\n",
    "published_pipeline_run_via_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "published_pipeline_run_via_rest.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Finish\n",
    "\n",
    "Disable created PipelineEndpoint and PublishedPipeline in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_endpoint.disable()\n",
    "published_pipeline.disable()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "category": "tutorial",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "Custom"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "How to use DataPath as a PipelineParameter",
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "order_index": 13,
  "star_tag": [
   "featured"
  ],
  "tags": [
   "None"
  ],
  "task": "Demonstrates the use of DataPath as a PipelineParameter"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
