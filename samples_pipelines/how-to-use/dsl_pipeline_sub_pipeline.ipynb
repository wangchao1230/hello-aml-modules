{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.pipeline.wrapper import Module, dsl\n",
    "from azureml.pipeline.wrapper._dataset import get_global_dataset_by_path\n",
    "from external_sub_pipeline import external_sub_pipeline0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module\n",
    "execute_python_script_module = Module.load(ws, namespace='azureml', name='Execute Python Script')\n",
    "\n",
    "# Basic Module\n",
    "basic_yaml_file = os.path.join('modules', 'hello_world', 'module_spec.yaml')\n",
    "basic_module_func = Module.from_yaml(ws, basic_yaml_file)\n",
    "\n",
    "# MPI Module\n",
    "mpi_yaml_file = os.path.join('modules', 'mpi_module', 'module_spec.yaml')\n",
    "mpi_module_func = Module.from_yaml(ws, mpi_yaml_file)\n",
    "\n",
    "# Parallel Module\n",
    "parallel_yaml_file = os.path.join('modules', 'parallel_module', 'copy_files.yaml')\n",
    "parallel_module_func = Module.from_yaml(ws, parallel_yaml_file)\n",
    "\n",
    "# TODO: Dataset\n",
    "blob_input_data = get_global_dataset_by_path(ws, 'Automobile_price_data', 'GenericCSV/Automobile_price_data_(Raw)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_name = 'aml_module_training_data'\n",
    "\n",
    "if training_data_name not in ws.datasets:\n",
    "    print('Registering a training dataset for sample pipeline ...')\n",
    "    train_data = Dataset.File.from_files(path=['https://dprepdata.blob.core.windows.net/demo/Titanic.csv'])\n",
    "    train_data.register(workspace=ws,\n",
    "                        name=training_data_name,\n",
    "                        description='Training data (just for illustrative purpose)')\n",
    "    print('Registerd')\n",
    "else:\n",
    "    train_data = ws.datasets[training_data_name]\n",
    "    print('Training dataset found in workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='sub0 graph', description='sub0')\n",
    "def sub_pipeline0(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        # should be pipeline input\n",
    "        dataset1=input,\n",
    "    )\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=module1.outputs.result_dataset,\n",
    "    )\n",
    "    return module2.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='sub1 graph', description='sub1')\n",
    "def sub_pipeline1(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        dataset1=input\n",
    "    )\n",
    "    sub0 = sub_pipeline0(module1.outputs.result_dataset)\n",
    "    return sub0.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='sub2 graph', description='sub1')\n",
    "def sub_pipeline2(input):\n",
    "    module1 = execute_python_script_module(\n",
    "        dataset1=input\n",
    "    )\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=module1.outputs.result_dataset,\n",
    "        dataset2=blob_input_data\n",
    "    )\n",
    "    module3 = execute_python_script_module(\n",
    "        dataset1=input,\n",
    "        dataset2=module2.outputs.result_dataset\n",
    "    )\n",
    "    module4 = execute_python_script_module(\n",
    "        dataset1=train_data,\n",
    "        dataset2=module3.outputs.result_dataset\n",
    "    )\n",
    "    sub0 = sub_pipeline0(module4.outputs.result_dataset)\n",
    "    return sub0.outputs\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='parent graph', description='parent', default_compute_target=\"aml-compute\")\n",
    "def parent_pipeline():\n",
    "    @dsl.pipeline(name='different type module', description='different type module', default_compute_target=\"aml-compute\")\n",
    "    def diff_type_pipeline(input, str_param, int_param, bool_param, enun_param):\n",
    "        basic_module = basic_module_func(\n",
    "            input_path=input,\n",
    "            string_parameter= str_param,\n",
    "            int_parameter= int_param,\n",
    "            boolean_parameter = bool_param,\n",
    "            enum_parameter=enun_param\n",
    "        )\n",
    "\n",
    "        # mpi module\n",
    "        mpi_module = mpi_module_func(\n",
    "            input_path=basic_module.outputs.output_path,\n",
    "            string_parameter= str_param,\n",
    "            int_parameter= int_param,\n",
    "            boolean_parameter = bool_param,\n",
    "            enum_parameter=enun_param\n",
    "        )\n",
    "        mpi_module.runsettings.configure(process_count_per_node=1, node_count=1)\n",
    "\n",
    "        # parallel module\n",
    "        parallel_module = parallel_module_func(\n",
    "            input_folder=mpi_module.outputs.output_path\n",
    "        )\n",
    "        parallel_module.runsettings.node_count = 1\n",
    "        return mpi_module.outputs\n",
    "\n",
    "    sub0 = diff_type_pipeline(blob_input_data, 'str_param', 5, True, \"option2\")\n",
    "    sub1 = sub_pipeline1(sub0.outputs.output_path)\n",
    "    sub2 = sub_pipeline2(sub1.outputs.result_dataset)\n",
    "    module2 = execute_python_script_module(\n",
    "        dataset1=sub2.outputs.result_dataset,\n",
    "        dataset2=train_data,\n",
    "    )\n",
    "\n",
    "    external = external_sub_pipeline0(sub1.outputs.result_dataset)\n",
    "    return module2.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = parent_pipeline()\n",
    "pipeline1.validate()\n",
    "\n",
    "run = pipeline1.submit(\n",
    "    experiment_name='module_SDK_test'\n",
    ")\n",
    "run.wait_for_completion()\n",
    "\n",
    "pipeline1.save(\n",
    "    experiment_name='module_SDK_test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline1 = parent_pipeline()\n",
    "#run = pipeline1.run(experiment_name='module_SDK_test', show_output=True, show_graph=True)\n",
    "#run"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "clwan"
   }
  ],
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('module-vscode': conda)",
   "language": "python",
   "name": "python37664bitmodulevscodeconda0e2bfe5a047b46e2954835fbaeea0018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
